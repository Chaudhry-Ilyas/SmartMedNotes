from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import logging
import os
# Import your custom modules
from models.phi3 import retrieve_faiss_docs, generate_response
from models.summarizer import summarize_context

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Check required files before starting the app
if not os.path.exists("processed_data/combined_faiss_index.faiss"):
    logging.error("FAISS index not found! Please generate it before deployment.")
    raise FileNotFoundError("FAISS index is missing at processed_data/combined_faiss_index.faiss")

if not os.path.exists("Phi-3-mini-4k-instruct-q4.gguf"):
    logging.error("Fine-tuned GGUF model file not found!")
    raise FileNotFoundError("GGUF model is missing at Phi-3-mini-4k-instruct-q4.gguf")

# Initialize FastAPI
app = FastAPI()

# Input schema for POST request
class QueryRequest(BaseModel):
    query: str

@app.get("/")
async def home():
    return {"message": "‚úÖ FastAPI backend is running successfully!"}

@app.post("/rag")
async def rag_pipeline(request: QueryRequest):
    user_query = request.query.strip()

    if not user_query:
        logging.warning("‚ùó Empty query received.")
        raise HTTPException(status_code=400, detail="Query cannot be empty.")

    logging.info(f"üì• Query received: {user_query}")

    # Step 1: Retrieve top documents
    retrieved_docs = retrieve_faiss_docs(user_query)
    logging.info(f"üìö Retrieved {len(retrieved_docs)} documents.")

    if not retrieved_docs:
        logging.warning("‚ö†Ô∏è No relevant documents found.")
        return {"query": user_query, "response": "Sorry, no relevant medical information found."}

    # Step 2: Summarize context using T5-small
    combined_context = " ".join(retrieved_docs)
    summarized_context = summarize_context(combined_context)
    logging.info("üìù Context summarized.")

    # Step 3: Generate response from LLM with full output
    final_response = generate_response(user_query)
    logging.info("ü§ñ Response generated by LLM.")

    return {"query": user_query, "response": final_response}

# Run with Uvicorn
def start():
    uvicorn.run(app, host="0.0.0.0", port=7860)

if __name__ == "__main__":
    start()
